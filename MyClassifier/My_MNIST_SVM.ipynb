{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte'\n",
    "                                % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte'\n",
    "                               % kind)\n",
    "\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\",\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "#### Loading the data\n",
    "\n",
    "X_train, y_train = load_mnist('./data', kind='newtrain')\n",
    "X_test, y_test = load_mnist('./data', kind='new1k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-patterns-idx3-ubyte'\n",
    "                               % kind)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\",\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(60000, 784)\n",
    "\n",
    "    return images\n",
    "\n",
    "X_testall = load_mnist('./data', kind='testall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sample_number:\t:80000, column_number:784\n",
      "test_sample_number:\t:10000, column_number:784\n",
      "testall_sample_number\t:60000, column_number:784\n"
     ]
    }
   ],
   "source": [
    "print('train_sample_number:\\t:%d, column_number:%d' %(X_train.shape[0], X_train.shape[1]))\n",
    "print('test_sample_number:\\t:%d, column_number:%d' %(X_test.shape[0], X_test.shape[1]))\n",
    "print('testall_sample_number\\t:%d, column_number:%d' %(X_testall.shape[0], X_testall.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train/255\n",
    "X_test=X_test/255\n",
    "X_testall=X_testall/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myClassifier(object):    \n",
    "    \"\"\"\n",
    "    ovr\n",
    "    \"\"\"\n",
    "    def __init__(self, C=1000, eta=0.01, batch_size=60, epochs=200, epsilon=1e-8, \n",
    "                 shuffle=True, params=None, w=0, b=0):\n",
    "        self.C = C\n",
    "        self.eta = eta\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.epsilon = epsilon\n",
    "        self.class_num = 0\n",
    "        self.shuffle = shuffle\n",
    "        self.update_count = 0\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "#         self.params['aver_w'] = w\n",
    "#         self.params['aver_b'] = b\n",
    "        \n",
    "    def fit(self, X, y, params=None, w=0, b=0, testscore = None, eval_score=None):\n",
    "        # X_num = m, X_fea = n\n",
    "        # m = np.shape(X)[0], n = np.shape(X)[1]\n",
    "        \n",
    "        X_num, X_fea = np.shape(X)\n",
    "        #X_num=60000 X_fea=28*28\n",
    "        self.class_num=len(np.unique(y))\n",
    "        #class_num=10\n",
    "        \n",
    "        if params is None:\n",
    "            print('fit params=None')\n",
    "            self.params = {\n",
    "                'w': np.random.randn(X_fea, self.class_num), #(10, 784) 정규분포난수\n",
    "                'b': np.random.randn(1, self.class_num),\n",
    "                'w_': np.random.randn(X_fea, self.class_num),\n",
    "                'b_': np.random.randn(1, self.class_num),\n",
    "                'tmpw': 0,\n",
    "                'tmpb': 0\n",
    "            }\n",
    "        cnt=1\n",
    "        if eval_score is None:\n",
    "            self.score_val = 0\n",
    "                \n",
    "        for Xi in range(self.epochs):\n",
    "            s_data, s_labels = self.shuffling(X, y)\n",
    "            encoded_y=self.encoding(s_labels)\n",
    "            avg_loss = 0\n",
    "            batch_count = int(X_num / self.batch_size)\n",
    "            for t in range(int(batch_count)):\n",
    "#               self.params['tmpw'] = temp_w, self.params['tmpb'] = temp_b\n",
    "                batch_X, batch_y, bs=self.batching(s_data, encoded_y, t)\n",
    "                batch_X = np.reshape(batch_X, (bs, X_fea))\n",
    "                batch_y = np.reshape(batch_y, (bs, self.class_num))\n",
    "                z = self.net_input(batch_X)\n",
    "                loss = self.hinge_loss(batch_y, z)\n",
    "                self.update_w_b(batch_X, batch_y, z, bs, cnt)\n",
    "                cnt+=1\n",
    "                avg_loss += loss\n",
    "                self.update_count += 1\n",
    "##ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "# aver_w = w_ , w_ = w\n",
    "            self.params['tmpw'] = (cnt * (cnt/(cnt+1)) * \n",
    "                                   self.params['w_'] + (1/(cnt+1))*self.params['w'])\n",
    "            self.params['tmpb'] = (cnt * (cnt/(cnt+1)) * \n",
    "                                   self.params['b_'] + (1/(cnt+1))*self.params['b'])\n",
    "            prev_score = self.score_val\n",
    "            pres_score = self.score(X, y)\n",
    "            print(\"epochs: \", Xi)\n",
    "            print(\"prev_score: \", prev_score)\n",
    "            print(\"pres_score: \", pres_score,\"\\n\")\n",
    "            if prev_score < pres_score:\n",
    "                self.score_val = pres_score\n",
    "            if self.det_weight(X, y, 1) < self.det_weight(X, y): # temp_w, temp_b\n",
    "                self.params['w_'] = self.params['tmpw']\n",
    "                self.params['b_'] = self.params['tmpb']\n",
    "            avg_loss /= batch_count\n",
    "        return self\n",
    "    \n",
    "    def det_weight(self, X, y, aver=0):\n",
    "        if aver:\n",
    "            w1 = self.params['w_']\n",
    "            b1 = self.params['b_']\n",
    "        else:\n",
    "            w1 = self.params['tmpw']\n",
    "            b1 = self.params['tmpb']\n",
    "        temp = np.dot(X, w1) + b1\n",
    "#         temp = temp.T\n",
    "        pred = np.argmax(temp, axis=1)\n",
    "        sco = np.mean(pred == y)\n",
    "        return sco\n",
    "    \n",
    "    def update_w_b(self, batch_X, batch_y, z, bs, cnt):\n",
    "        n = np.shape(batch_X)[1]  # num of features\n",
    "        delta_w = np.zeros(self.params['w'].shape)\n",
    "        delta_b = np.zeros(self.params['b'].shape)\n",
    "        z = np.reshape(z, (bs, self.class_num))\n",
    "        temp = 1 - np.multiply(batch_y, z)\n",
    "        temp[temp <= 0] = 0\n",
    "        temp[temp > 0] = 1\n",
    "        y_temp = np.multiply(batch_y, temp.reshape(bs, self.class_num))\n",
    "        delta_w = -(1 / bs) * np.matmul(batch_X.T, y_temp) + (1 / self.C) * self.params['w']\n",
    "        delta_b = -(1 / bs) * np.sum(y_temp, axis=0)\n",
    "        self.params['w'] = self.params['w'] - (self.eta / (1 + self.epsilon * cnt)) * delta_w\n",
    "        self.params['b'] = self.params['b'] - (self.eta / (1 + self.epsilon * cnt)) * delta_b\n",
    "        \n",
    "        return self.params\n",
    "    \n",
    "    def hinge_loss(self, y, z):\n",
    "        loss = 1 - np.multiply(y, z)\n",
    "        loss[loss < 0] = 0\n",
    "        loss = np.mean(loss)\n",
    "        return loss\n",
    "    \n",
    "    def net_input(self, X):  # net_input() = forward_prop(), generate z\n",
    "        z = np.matmul(X, self.params['w']) + self.params['b']\n",
    "        return z\n",
    "\n",
    "    def encoding(self, y):\n",
    "        encoded_y=-1*np.ones((np.shape(y)[0],self.class_num))\n",
    "        for i in range(np.shape(y)[0]):\n",
    "            encoded_y[i,y[i]] = 1\n",
    "        return encoded_y\n",
    "                \n",
    "    def shuffling(self, X, y):\n",
    "        temp_s=list(zip(X,y))\n",
    "        random.shuffle(temp_s)\n",
    "        X,y=zip(*temp_s)\n",
    "        return X,y\n",
    "    \n",
    "    def batching(self, X, y, t):                         \n",
    "        batch_X = X[t * self.batch_size : min(len(X), (t+1) * self.batch_size)]\n",
    "        batch_y = y[t * self.batch_size : min(len(X), (t+1) * self.batch_size)]\n",
    "        last_size = min(len(X), (t+1) * self.batch_size) - t * self.batch_size\n",
    "        \n",
    "        return batch_X, batch_y,last_size\n",
    "    \n",
    "    def predict(self, X):\n",
    "        m = np.shape(X)[0]\n",
    "        class_score = self.net_input(X)  # return z\n",
    "        pred = np.argmax(class_score, axis=1)\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        score = np.mean(pred == y)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'C':self.C, 'batch_size':self.batch_size, 'epochs':self.epochs,\n",
    "               'eta': self.eta, 'w':self.params['w_'], 'b':self.params['b_']}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def test(self, X, w, b):\n",
    "        z = np.matmul(X, np.array(w)) + np.array(b)\n",
    "        p = np.argmax(z, axis=1)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit params=None\n",
      "epochs:  0\n",
      "prev_score:  0\n",
      "pres_score:  0.52655 \n",
      "\n",
      "epochs:  1\n",
      "prev_score:  0.52655\n",
      "pres_score:  0.66385 \n",
      "\n",
      "epochs:  2\n",
      "prev_score:  0.66385\n",
      "pres_score:  0.7223875 \n",
      "\n",
      "epochs:  3\n",
      "prev_score:  0.7223875\n",
      "pres_score:  0.755325 \n",
      "\n",
      "epochs:  4\n",
      "prev_score:  0.755325\n",
      "pres_score:  0.777475 \n",
      "\n",
      "epochs:  5\n",
      "prev_score:  0.777475\n",
      "pres_score:  0.7923375 \n",
      "\n",
      "epochs:  6\n",
      "prev_score:  0.7923375\n",
      "pres_score:  0.8035125 \n",
      "\n",
      "epochs:  7\n",
      "prev_score:  0.8035125\n",
      "pres_score:  0.8132125 \n",
      "\n",
      "epochs:  8\n",
      "prev_score:  0.8132125\n",
      "pres_score:  0.820725 \n",
      "\n",
      "epochs:  9\n",
      "prev_score:  0.820725\n",
      "pres_score:  0.827175 \n",
      "\n",
      "epochs:  10\n",
      "prev_score:  0.827175\n",
      "pres_score:  0.832875 \n",
      "\n",
      "epochs:  11\n",
      "prev_score:  0.832875\n",
      "pres_score:  0.8373625 \n",
      "\n",
      "epochs:  12\n",
      "prev_score:  0.8373625\n",
      "pres_score:  0.84145 \n",
      "\n",
      "epochs:  13\n",
      "prev_score:  0.84145\n",
      "pres_score:  0.84495 \n",
      "\n",
      "epochs:  14\n",
      "prev_score:  0.84495\n",
      "pres_score:  0.847325 \n",
      "\n",
      "epochs:  15\n",
      "prev_score:  0.847325\n",
      "pres_score:  0.8506625 \n",
      "\n",
      "epochs:  16\n",
      "prev_score:  0.8506625\n",
      "pres_score:  0.8532875 \n",
      "\n",
      "epochs:  17\n",
      "prev_score:  0.8532875\n",
      "pres_score:  0.8560875 \n",
      "\n",
      "epochs:  18\n",
      "prev_score:  0.8560875\n",
      "pres_score:  0.858325 \n",
      "\n",
      "epochs:  19\n",
      "prev_score:  0.858325\n",
      "pres_score:  0.860575 \n",
      "\n",
      "epochs:  20\n",
      "prev_score:  0.860575\n",
      "pres_score:  0.8620375 \n",
      "\n",
      "epochs:  21\n",
      "prev_score:  0.8620375\n",
      "pres_score:  0.86425 \n",
      "\n",
      "epochs:  22\n",
      "prev_score:  0.86425\n",
      "pres_score:  0.865925 \n",
      "\n",
      "epochs:  23\n",
      "prev_score:  0.865925\n",
      "pres_score:  0.867725 \n",
      "\n",
      "epochs:  24\n",
      "prev_score:  0.867725\n",
      "pres_score:  0.8691375 \n",
      "\n",
      "epochs:  25\n",
      "prev_score:  0.8691375\n",
      "pres_score:  0.870425 \n",
      "\n",
      "epochs:  26\n",
      "prev_score:  0.870425\n",
      "pres_score:  0.87185 \n",
      "\n",
      "epochs:  27\n",
      "prev_score:  0.87185\n",
      "pres_score:  0.8728375 \n",
      "\n",
      "epochs:  28\n",
      "prev_score:  0.8728375\n",
      "pres_score:  0.8752125 \n",
      "\n",
      "epochs:  29\n",
      "prev_score:  0.8752125\n",
      "pres_score:  0.8759 \n",
      "\n",
      "epochs:  30\n",
      "prev_score:  0.8759\n",
      "pres_score:  0.87705 \n",
      "\n",
      "epochs:  31\n",
      "prev_score:  0.87705\n",
      "pres_score:  0.8781375 \n",
      "\n",
      "epochs:  32\n",
      "prev_score:  0.8781375\n",
      "pres_score:  0.879 \n",
      "\n",
      "epochs:  33\n",
      "prev_score:  0.879\n",
      "pres_score:  0.88015 \n",
      "\n",
      "epochs:  34\n",
      "prev_score:  0.88015\n",
      "pres_score:  0.8814125 \n",
      "\n",
      "epochs:  35\n",
      "prev_score:  0.8814125\n",
      "pres_score:  0.8819625 \n",
      "\n",
      "epochs:  36\n",
      "prev_score:  0.8819625\n",
      "pres_score:  0.8832 \n",
      "\n",
      "epochs:  37\n",
      "prev_score:  0.8832\n",
      "pres_score:  0.884025 \n",
      "\n",
      "epochs:  38\n",
      "prev_score:  0.884025\n",
      "pres_score:  0.88485 \n",
      "\n",
      "epochs:  39\n",
      "prev_score:  0.88485\n",
      "pres_score:  0.8858875 \n",
      "\n",
      "epochs:  40\n",
      "prev_score:  0.8858875\n",
      "pres_score:  0.88675 \n",
      "\n",
      "epochs:  41\n",
      "prev_score:  0.88675\n",
      "pres_score:  0.8872625 \n",
      "\n",
      "epochs:  42\n",
      "prev_score:  0.8872625\n",
      "pres_score:  0.88845 \n",
      "\n",
      "epochs:  43\n",
      "prev_score:  0.88845\n",
      "pres_score:  0.88865 \n",
      "\n",
      "epochs:  44\n",
      "prev_score:  0.88865\n",
      "pres_score:  0.8894375 \n",
      "\n",
      "epochs:  45\n",
      "prev_score:  0.8894375\n",
      "pres_score:  0.8902625 \n",
      "\n",
      "epochs:  46\n",
      "prev_score:  0.8902625\n",
      "pres_score:  0.8909125 \n",
      "\n",
      "epochs:  47\n",
      "prev_score:  0.8909125\n",
      "pres_score:  0.8917 \n",
      "\n",
      "epochs:  48\n",
      "prev_score:  0.8917\n",
      "pres_score:  0.892525 \n",
      "\n",
      "epochs:  49\n",
      "prev_score:  0.892525\n",
      "pres_score:  0.89305 \n",
      "\n",
      "epochs:  50\n",
      "prev_score:  0.89305\n",
      "pres_score:  0.8932375 \n",
      "\n",
      "epochs:  51\n",
      "prev_score:  0.8932375\n",
      "pres_score:  0.8937125 \n",
      "\n",
      "epochs:  52\n",
      "prev_score:  0.8937125\n",
      "pres_score:  0.894675 \n",
      "\n",
      "epochs:  53\n",
      "prev_score:  0.894675\n",
      "pres_score:  0.8951125 \n",
      "\n",
      "epochs:  54\n",
      "prev_score:  0.8951125\n",
      "pres_score:  0.8952875 \n",
      "\n",
      "epochs:  55\n",
      "prev_score:  0.8952875\n",
      "pres_score:  0.8959375 \n",
      "\n",
      "epochs:  56\n",
      "prev_score:  0.8959375\n",
      "pres_score:  0.8963875 \n",
      "\n",
      "epochs:  57\n",
      "prev_score:  0.8963875\n",
      "pres_score:  0.8969625 \n",
      "\n",
      "epochs:  58\n",
      "prev_score:  0.8969625\n",
      "pres_score:  0.8974375 \n",
      "\n",
      "epochs:  59\n",
      "prev_score:  0.8974375\n",
      "pres_score:  0.8980375 \n",
      "\n",
      "epochs:  60\n",
      "prev_score:  0.8980375\n",
      "pres_score:  0.8985 \n",
      "\n",
      "epochs:  61\n",
      "prev_score:  0.8985\n",
      "pres_score:  0.8986875 \n",
      "\n",
      "epochs:  62\n",
      "prev_score:  0.8986875\n",
      "pres_score:  0.8985875 \n",
      "\n",
      "epochs:  63\n",
      "prev_score:  0.8986875\n",
      "pres_score:  0.898925 \n",
      "\n",
      "epochs:  64\n",
      "prev_score:  0.898925\n",
      "pres_score:  0.8999 \n",
      "\n",
      "epochs:  65\n",
      "prev_score:  0.8999\n",
      "pres_score:  0.899525 \n",
      "\n",
      "epochs:  66\n",
      "prev_score:  0.8999\n",
      "pres_score:  0.8996 \n",
      "\n",
      "epochs:  67\n",
      "prev_score:  0.8999\n",
      "pres_score:  0.9004625 \n",
      "\n",
      "epochs:  68\n",
      "prev_score:  0.9004625\n",
      "pres_score:  0.900575 \n",
      "\n",
      "epochs:  69\n",
      "prev_score:  0.900575\n",
      "pres_score:  0.9004375 \n",
      "\n",
      "epochs:  70\n",
      "prev_score:  0.900575\n",
      "pres_score:  0.901125 \n",
      "\n",
      "epochs:  71\n",
      "prev_score:  0.901125\n",
      "pres_score:  0.9011125 \n",
      "\n",
      "epochs:  72\n",
      "prev_score:  0.901125\n",
      "pres_score:  0.9015875 \n",
      "\n",
      "epochs:  73\n",
      "prev_score:  0.9015875\n",
      "pres_score:  0.901525 \n",
      "\n",
      "epochs:  74\n",
      "prev_score:  0.9015875\n",
      "pres_score:  0.9022 \n",
      "\n",
      "epochs:  75\n",
      "prev_score:  0.9022\n",
      "pres_score:  0.9024875 \n",
      "\n",
      "epochs:  76\n",
      "prev_score:  0.9024875\n",
      "pres_score:  0.9025875 \n",
      "\n",
      "epochs:  77\n",
      "prev_score:  0.9025875\n",
      "pres_score:  0.9028 \n",
      "\n",
      "epochs:  78\n",
      "prev_score:  0.9028\n",
      "pres_score:  0.9029375 \n",
      "\n",
      "epochs:  79\n",
      "prev_score:  0.9029375\n",
      "pres_score:  0.903225 \n",
      "\n",
      "epochs:  80\n",
      "prev_score:  0.903225\n",
      "pres_score:  0.9030375 \n",
      "\n",
      "epochs:  81\n",
      "prev_score:  0.903225\n",
      "pres_score:  0.903425 \n",
      "\n",
      "epochs:  82\n",
      "prev_score:  0.903425\n",
      "pres_score:  0.9038 \n",
      "\n",
      "epochs:  83\n",
      "prev_score:  0.9038\n",
      "pres_score:  0.9038875 \n",
      "\n",
      "epochs:  84\n",
      "prev_score:  0.9038875\n",
      "pres_score:  0.9038625 \n",
      "\n",
      "epochs:  85\n",
      "prev_score:  0.9038875\n",
      "pres_score:  0.9042625 \n",
      "\n",
      "epochs:  86\n",
      "prev_score:  0.9042625\n",
      "pres_score:  0.9044125 \n",
      "\n",
      "epochs:  87\n",
      "prev_score:  0.9044125\n",
      "pres_score:  0.9045875 \n",
      "\n",
      "epochs:  88\n",
      "prev_score:  0.9045875\n",
      "pres_score:  0.9047125 \n",
      "\n",
      "epochs:  89\n",
      "prev_score:  0.9047125\n",
      "pres_score:  0.904425 \n",
      "\n",
      "epochs:  90\n",
      "prev_score:  0.9047125\n",
      "pres_score:  0.904675 \n",
      "\n",
      "epochs:  91\n",
      "prev_score:  0.9047125\n",
      "pres_score:  0.9051875 \n",
      "\n",
      "epochs:  92\n",
      "prev_score:  0.9051875\n",
      "pres_score:  0.9049375 \n",
      "\n",
      "epochs:  93\n",
      "prev_score:  0.9051875\n",
      "pres_score:  0.9057875 \n",
      "\n",
      "epochs:  94\n",
      "prev_score:  0.9057875\n",
      "pres_score:  0.9055375 \n",
      "\n",
      "epochs:  95\n",
      "prev_score:  0.9057875\n",
      "pres_score:  0.9055375 \n",
      "\n",
      "epochs:  96\n",
      "prev_score:  0.9057875\n",
      "pres_score:  0.9058375 \n",
      "\n",
      "epochs:  97\n",
      "prev_score:  0.9058375\n",
      "pres_score:  0.905725 \n",
      "\n",
      "epochs:  98\n",
      "prev_score:  0.9058375\n",
      "pres_score:  0.9056375 \n",
      "\n",
      "epochs:  99\n",
      "prev_score:  0.9058375\n",
      "pres_score:  0.9062625 \n",
      "\n",
      "epochs:  100\n",
      "prev_score:  0.9062625\n",
      "pres_score:  0.9057875 \n",
      "\n",
      "epochs:  101\n",
      "prev_score:  0.9062625\n",
      "pres_score:  0.906125 \n",
      "\n",
      "epochs:  102\n",
      "prev_score:  0.9062625\n",
      "pres_score:  0.906575 \n",
      "\n",
      "epochs:  103\n",
      "prev_score:  0.906575\n",
      "pres_score:  0.9067125 \n",
      "\n",
      "epochs:  104\n",
      "prev_score:  0.9067125\n",
      "pres_score:  0.9063125 \n",
      "\n",
      "epochs:  105\n",
      "prev_score:  0.9067125\n",
      "pres_score:  0.90645 \n",
      "\n",
      "epochs:  106\n",
      "prev_score:  0.9067125\n",
      "pres_score:  0.9065125 \n",
      "\n",
      "epochs:  107\n",
      "prev_score:  0.9067125\n",
      "pres_score:  0.9066125 \n",
      "\n",
      "epochs:  108\n",
      "prev_score:  0.9067125\n",
      "pres_score:  0.9067 \n",
      "\n",
      "epochs:  109\n",
      "prev_score:  0.9067125\n",
      "pres_score:  0.90715 \n",
      "\n",
      "epochs:  110\n",
      "prev_score:  0.90715\n",
      "pres_score:  0.9068625 \n",
      "\n",
      "epochs:  111\n",
      "prev_score:  0.90715\n",
      "pres_score:  0.9069625 \n",
      "\n",
      "epochs:  112\n",
      "prev_score:  0.90715\n",
      "pres_score:  0.907025 \n",
      "\n",
      "epochs:  113\n",
      "prev_score:  0.90715\n",
      "pres_score:  0.9070625 \n",
      "\n",
      "epochs:  114\n",
      "prev_score:  0.90715\n",
      "pres_score:  0.90745 \n",
      "\n",
      "epochs:  115\n",
      "prev_score:  0.90745\n",
      "pres_score:  0.9075875 \n",
      "\n",
      "epochs:  116\n",
      "prev_score:  0.9075875\n",
      "pres_score:  0.9073125 \n",
      "\n",
      "epochs:  117\n",
      "prev_score:  0.9075875\n",
      "pres_score:  0.9070625 \n",
      "\n",
      "epochs:  118\n",
      "prev_score:  0.9075875\n",
      "pres_score:  0.907325 \n",
      "\n",
      "epochs:  119\n",
      "prev_score:  0.9075875\n",
      "pres_score:  0.907925 \n",
      "\n",
      "epochs:  120\n",
      "prev_score:  0.907925\n",
      "pres_score:  0.9075125 \n",
      "\n",
      "epochs:  121\n",
      "prev_score:  0.907925\n",
      "pres_score:  0.90805 \n",
      "\n",
      "epochs:  122\n",
      "prev_score:  0.90805\n",
      "pres_score:  0.9078625 \n",
      "\n",
      "epochs:  123\n",
      "prev_score:  0.90805\n",
      "pres_score:  0.9078375 \n",
      "\n",
      "epochs:  124\n",
      "prev_score:  0.90805\n",
      "pres_score:  0.9081125 \n",
      "\n",
      "epochs:  125\n",
      "prev_score:  0.9081125\n",
      "pres_score:  0.9081375 \n",
      "\n",
      "epochs:  126\n",
      "prev_score:  0.9081375\n",
      "pres_score:  0.907825 \n",
      "\n",
      "epochs:  127\n",
      "prev_score:  0.9081375\n",
      "pres_score:  0.9080125 \n",
      "\n",
      "epochs:  128\n",
      "prev_score:  0.9081375\n",
      "pres_score:  0.908075 \n",
      "\n",
      "epochs:  129\n",
      "prev_score:  0.9081375\n",
      "pres_score:  0.9078875 \n",
      "\n",
      "epochs:  130\n",
      "prev_score:  0.9081375\n",
      "pres_score:  0.9078875 \n",
      "\n",
      "epochs:  131\n",
      "prev_score:  0.9081375\n",
      "pres_score:  0.9081875 \n",
      "\n",
      "epochs:  132\n",
      "prev_score:  0.9081875\n",
      "pres_score:  0.9084 \n",
      "\n",
      "epochs:  133\n",
      "prev_score:  0.9084\n",
      "pres_score:  0.908225 \n",
      "\n",
      "epochs:  134\n",
      "prev_score:  0.9084\n",
      "pres_score:  0.908075 \n",
      "\n",
      "epochs:  135\n",
      "prev_score:  0.9084\n",
      "pres_score:  0.9087 \n",
      "\n",
      "epochs:  136\n",
      "prev_score:  0.9087\n",
      "pres_score:  0.907875 \n",
      "\n",
      "epochs:  137\n",
      "prev_score:  0.9087\n",
      "pres_score:  0.9083375 \n",
      "\n",
      "epochs:  138\n",
      "prev_score:  0.9087\n",
      "pres_score:  0.9083125 \n",
      "\n",
      "epochs:  139\n",
      "prev_score:  0.9087\n",
      "pres_score:  0.90865 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:  140\n",
      "prev_score:  0.9087\n",
      "pres_score:  0.9087125 \n",
      "\n",
      "epochs:  141\n",
      "prev_score:  0.9087125\n",
      "pres_score:  0.908375 \n",
      "\n",
      "epochs:  142\n",
      "prev_score:  0.9087125\n",
      "pres_score:  0.9085 \n",
      "\n",
      "epochs:  143\n",
      "prev_score:  0.9087125\n",
      "pres_score:  0.9085375 \n",
      "\n",
      "epochs:  144\n",
      "prev_score:  0.9087125\n",
      "pres_score:  0.9088125 \n",
      "\n",
      "epochs:  145\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.908125 \n",
      "\n",
      "epochs:  146\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.908225 \n",
      "\n",
      "epochs:  147\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.908525 \n",
      "\n",
      "epochs:  148\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.9084125 \n",
      "\n",
      "epochs:  149\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.9082625 \n",
      "\n",
      "epochs:  150\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.9083 \n",
      "\n",
      "epochs:  151\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.908575 \n",
      "\n",
      "epochs:  152\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.908275 \n",
      "\n",
      "epochs:  153\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.9087 \n",
      "\n",
      "epochs:  154\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.9084625 \n",
      "\n",
      "epochs:  155\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.9084875 \n",
      "\n",
      "epochs:  156\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.9086125 \n",
      "\n",
      "epochs:  157\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.9084375 \n",
      "\n",
      "epochs:  158\n",
      "prev_score:  0.9088125\n",
      "pres_score:  0.9088625 \n",
      "\n",
      "epochs:  159\n",
      "prev_score:  0.9088625\n",
      "pres_score:  0.908575 \n",
      "\n",
      "epochs:  160\n",
      "prev_score:  0.9088625\n",
      "pres_score:  0.9087625 \n",
      "\n",
      "epochs:  161\n",
      "prev_score:  0.9088625\n",
      "pres_score:  0.9089125 \n",
      "\n",
      "epochs:  162\n",
      "prev_score:  0.9089125\n",
      "pres_score:  0.9086125 \n",
      "\n",
      "epochs:  163\n",
      "prev_score:  0.9089125\n",
      "pres_score:  0.9080375 \n",
      "\n",
      "epochs:  164\n",
      "prev_score:  0.9089125\n",
      "pres_score:  0.9091375 \n",
      "\n",
      "epochs:  165\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9084375 \n",
      "\n",
      "epochs:  166\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9086875 \n",
      "\n",
      "epochs:  167\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.90845 \n",
      "\n",
      "epochs:  168\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9086375 \n",
      "\n",
      "epochs:  169\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.908825 \n",
      "\n",
      "epochs:  170\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9086625 \n",
      "\n",
      "epochs:  171\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9090375 \n",
      "\n",
      "epochs:  172\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9083875 \n",
      "\n",
      "epochs:  173\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9086125 \n",
      "\n",
      "epochs:  174\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9087125 \n",
      "\n",
      "epochs:  175\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.908675 \n",
      "\n",
      "epochs:  176\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9083 \n",
      "\n",
      "epochs:  177\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.908775 \n",
      "\n",
      "epochs:  178\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9090125 \n",
      "\n",
      "epochs:  179\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.90885 \n",
      "\n",
      "epochs:  180\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9087375 \n",
      "\n",
      "epochs:  181\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9084125 \n",
      "\n",
      "epochs:  182\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.908575 \n",
      "\n",
      "epochs:  183\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9087125 \n",
      "\n",
      "epochs:  184\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9086375 \n",
      "\n",
      "epochs:  185\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.908625 \n",
      "\n",
      "epochs:  186\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9087125 \n",
      "\n",
      "epochs:  187\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.90845 \n",
      "\n",
      "epochs:  188\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9090625 \n",
      "\n",
      "epochs:  189\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.908725 \n",
      "\n",
      "epochs:  190\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9089875 \n",
      "\n",
      "epochs:  191\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.908675 \n",
      "\n",
      "epochs:  192\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9089125 \n",
      "\n",
      "epochs:  193\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9080375 \n",
      "\n",
      "epochs:  194\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9088875 \n",
      "\n",
      "epochs:  195\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.908775 \n",
      "\n",
      "epochs:  196\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9089875 \n",
      "\n",
      "epochs:  197\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9089625 \n",
      "\n",
      "epochs:  198\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9089125 \n",
      "\n",
      "epochs:  199\n",
      "prev_score:  0.9091375\n",
      "pres_score:  0.9085375 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.myClassifier at 0x7f9b2237c2d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mine=myClassifier()\n",
    "mine.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000,\n",
       " 'batch_size': 60,\n",
       " 'epochs': 200,\n",
       " 'eta': 0.01,\n",
       " 'w': array([[ 1.0854312 ,  1.60474952,  1.29690399, ...,  0.1711392 ,\n",
       "          0.93949008,  1.25477187],\n",
       "        [-0.51138963, -0.40311992,  0.94269138, ...,  0.14857752,\n",
       "          0.5367502 ,  0.86845323],\n",
       "        [ 0.6607393 ,  0.15685686,  1.41669104, ..., -0.02112389,\n",
       "         -0.46241083, -0.72440239],\n",
       "        ...,\n",
       "        [-1.00923905,  0.33886948,  0.4260792 , ..., -0.29764807,\n",
       "         -0.22932422, -0.34106951],\n",
       "        [ 1.97990312,  1.62949041,  2.11518993, ..., -0.069971  ,\n",
       "         -0.61228839,  0.55258546],\n",
       "        [-0.46404495, -1.02294704, -1.02440946, ...,  1.44857499,\n",
       "         -0.57983122,  0.19809106]]),\n",
       " 'b': array([[ 0.78105322, -0.61673477,  1.04131276,  0.77108051, -0.22672777,\n",
       "         -2.02352649,  0.71710939, -1.70611866,  0.24443303,  0.33781992]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mine.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=mine.get_params()['w']\n",
    "b=mine.get_params()['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p=mine.test(X_testall,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file=open('/home/ryu/AI/MyClassifier/prediction.txt','w')\n",
    "for i in range(len(p)):\n",
    "    file.write('%s\\n' %p[i])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_= mine.get_params()\n",
    "\n",
    "w__=pd.DataFrame(list_['w'])\n",
    "w__.to_csv(\"final_w.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b__=pd.DataFrame(list_['b'])\n",
    "b__.to_csv(\"final_b.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_2w=pd.read_csv('/home/ryu/AI/MyClassifier/final_w.csv')\n",
    "list_2b=pd.read_csv('/home/ryu/AI/MyClassifier/final_b.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
