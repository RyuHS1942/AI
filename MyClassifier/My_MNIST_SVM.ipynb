{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels.idx1-ubyte'\n",
    "                                % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images.idx3-ubyte'\n",
    "                               % kind)\n",
    "\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\",\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "#### Loading the data\n",
    "\n",
    "X_train, y_train = load_mnist('./data', kind='train')\n",
    "X_test, y_test = load_mnist('./data', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sample_number:\t:60000, column_number:784\n",
      "test_sample_number\t:10000, column_number:784\n"
     ]
    }
   ],
   "source": [
    "print('train_sample_number:\\t:%d, column_number:%d' %(X_train.shape[0], X_train.shape[1]))\n",
    "print('test_sample_number\\t:%d, column_number:%d' %(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train/255\n",
    "X_test=X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class myClassifier(object):\n",
    "    \"\"\"\n",
    "    ovr\n",
    "    \"\"\"\n",
    "    def __init__(self, C=0.1, eta=0.001, batch_size=500, num_iter=25, epsilon=0.001, class_num=0, shuffle=True):\n",
    "        self.C=C\n",
    "        self.eta=eta\n",
    "        self.batch_size=batch_size\n",
    "        self.num_iter=num_iter\n",
    "        self.epsilon=epsilon\n",
    "        self.class_num=class_num\n",
    "        self.shuffle=shuffle\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X_num, X_fea = np.shape(X)\n",
    "        #X_num=60000 X_fea=28*28\n",
    "        self.class_num=len(np.unique(y))\n",
    "        #class_num=10\n",
    "        \n",
    "        w=np.random.rand(self.class_num,X_fea)\n",
    "        #w=[class_num][28*28]\n",
    "        aver_w=w\n",
    "        b=np.random.rand(self.class_num,1)\n",
    "        #b=[class_num][1]\n",
    "        aver_b=b\n",
    "        \n",
    "        if self.shuffle:\n",
    "            s_data, s_labels = self.shuffling(X, y)\n",
    "            #s_data[60000][28*28]\n",
    "            #s_labels[60000][1]\n",
    "        \n",
    "        encoded_y=self.encoding(s_labels)\n",
    "        #encoded_y[60000][class_num]\n",
    "        \n",
    "        cnt=0\n",
    "        batch_count=X_num/self.batch_size\n",
    "        for Xi in range(self.num_iter):\n",
    "            #minibatch training\n",
    "            for t in range(int(batch_count)):\n",
    "                ###sgd\n",
    "                batch_X, batch_y, bs=self.batching(s_data, encoded_y, t)\n",
    "                #batch_X[batch_size][784] batch_y[batch_size][class_num] last_size=?\n",
    "                \n",
    "                batch_X=np.reshape(batch_X,(bs,X_fea))\n",
    "                batch_y=np.reshape(batch_y,(bs,self.class_num))\n",
    "                \n",
    "                delta_w=np.zeros((self.class_num,X_fea))\n",
    "                #delta_w[class_num][28*28]\n",
    "                delta_b=np.zeros((self.class_num,1))\n",
    "                #delta_b[10][1]\n",
    "                \n",
    "                loss=self.hinge_loss(batch_X, batch_y, w, b)\n",
    "                #loss[batch_size][class_num]\n",
    "                loss=1-loss\n",
    "                \n",
    "                loss[loss<=0]=0\n",
    "                loss[loss>0]=1\n",
    "                \n",
    "                l_M_y=loss*batch_y\n",
    "                #l_M_y  =loss[bs][class_num] batch_y[bs][class_num]\n",
    "                \n",
    "                temp_w=np.dot(np.transpose(l_M_y),batch_X)\n",
    "                #temp[10][28*28]=batch_y[batch_size][28*28]\n",
    "                delta_w=-(1/bs)*np.array(temp_w)+(1/self.C)*np.array(w)\n",
    "                #delta_w[10][28*28]=c*temp[10][28*28]+c*w[10][28*28]\n",
    "                \n",
    "                temp_b=np.sum(np.transpose(l_M_y),axis=1)\n",
    "                temp_b=np.reshape(temp_b,(self.class_num,1))\n",
    "                #temp[class_num][1]\n",
    "                delta_b=-(1/bs) * temp_b\n",
    "                #delta_b[10][1]=c*[class_num][1]\n",
    "                cnt+=1\n",
    "            ###algorism\n",
    "            \n",
    "            w=np.array(w)-(self.eta*np.array(delta_w))\n",
    "            #w[class_num][28*28]\n",
    "            b=np.subtract(b,(self.eta * delta_b))\n",
    "            #b[class_num][1]\n",
    "            \n",
    "            temp_w=(cnt/cnt+1)*aver_w + (1/cnt+1)*w\n",
    "            temp_b=(cnt/cnt+1)*aver_b + (1/cnt+1)*b\n",
    "            \n",
    "            aver_w=np.where(aver_w>temp_w,aver_w,temp_w)\n",
    "            aver_b=np.where(aver_b>temp_b,aver_b,aver_b)\n",
    "            \n",
    "        return aver_w, aver_b\n",
    "           \n",
    "    def encoding(self, y):\n",
    "        encoded_y=-1*np.ones((np.shape(y)[0],self.class_num))\n",
    "        #encoded_y[60000][class_num]\n",
    "        for i in range(np.shape(y)[0]):\n",
    "            encoded_y[i,y[i]] = 1\n",
    "        return encoded_y\n",
    "                \n",
    "    def shuffling(self, X, y):\n",
    "        temp=list(zip(X,y))\n",
    "        random.shuffle(temp)\n",
    "        X,y=zip(*temp)\n",
    "        return X,y\n",
    "    \n",
    "    def batching(self, X, y, t):                         \n",
    "        batch_X=X[t*self.batch_size:min(len(X),(t+1)*self.batch_size)]\n",
    "        #batch_X[batch_size][28*28]\n",
    "        batch_y=y[t*self.batch_size:min(len(X),(t+1)*self.batch_size)]\n",
    "        #batch_y[batch_size][class_num]\n",
    "        last_size=min(len(X), (t+1)*self.batch_size)-t*self.batch_size\n",
    "        #last_size[size][28*28]\n",
    "        \n",
    "        return batch_X, batch_y,last_size\n",
    "    \n",
    "    def hinge_loss(self, X, y, w, b):\n",
    "        net_v=self.net_input(X,w)\n",
    "        #net_v[batch_size][class_num]\n",
    "        temp=np.array(net_v)+np.transpose(b)\n",
    "        #temp[batch_size][class_num]\n",
    "        loss=y*temp\n",
    "        #loss[batch_size][class_num]\n",
    "        return loss\n",
    "    \n",
    "    def net_input(self, X, w):\n",
    "        #X[batch_size][28*28] w[class_num][28*28]\n",
    "        return np.dot(X,np.transpose(w))#[batch_size][class_num]\n",
    "                      \n",
    "    def test(self,X,y,w,b):\n",
    "        net_v=self.net_input(X,w)\n",
    "        #net_v[batch_size][class_num]\n",
    "        temp=np.array(net_v)+np.transpose(b)\n",
    "        #temp[batch_size][class_num]\n",
    "        \n",
    "        predicted=np.argmax(temp,axis=1)\n",
    "        \n",
    "        correct=np.count_nonzero(np.where(predicted==y,1,0))\n",
    "        return correct\n",
    "    \n",
    "    def img(self, row, X, y, p):\n",
    "        image = np.zeros((28,28))\n",
    "        for i in range(0,28):\n",
    "            for j in range(0,28):\n",
    "                pix = 28*i+j\n",
    "                image[i,j] = X[row, pix]\n",
    "        plt.imshow(image, cmap = 'gray')\n",
    "        plt.title('%d)true_value: %d pridicted_value: %d' %(row+1, y[row], p))\n",
    "        plt.show()\n",
    "        return print(X[row,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySVM(object):\n",
    "    \"\"\"\n",
    "    1 vs 1 SVM (binary classification)\n",
    "    \"\"\"\n",
    "    def __init__(self, C=0.1, eta=0.001, batch_size=1, max_iter=25, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        constructor of MyBinarySVM class.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.eta = eta\n",
    "        self.C = C\n",
    "        self.epsilon = epsilon\n",
    "        self.num_classes = 0\n",
    "#         self.beta1 = 0.9\n",
    "#         self.beta2 = 0.99\n",
    "    \n",
    "    def fit(self, X, y=None, params=None):\n",
    "        \"\"\"\n",
    "        fit method for training svm\n",
    "        \n",
    "        Arguments:\n",
    "        --------------------------\n",
    "        X: image data. (60000, 784)\n",
    "        y: label data. (60000, 1)\n",
    "        \n",
    "        Returns:\n",
    "        --------------------------\n",
    "        Z: class score\n",
    "        \"\"\"\n",
    "\n",
    "        m = np.shape(X)[0] #행 개수 60000\n",
    "        n = np.shape(X)[1] #열 개수 784\n",
    "        self.num_classes = len(np.unique(y)) #클래스 수 = 10\n",
    "        \n",
    "        y_encoded = self.encode_y(y)\n",
    "        \n",
    "        # create weights.\n",
    "        if params is None:\n",
    "            self.params = {\n",
    "                'W': np.random.randn(n, self.num_classes), #(784,10) 정규분포난수\n",
    "                'b': np.random.randn(1, self.num_classes)\n",
    "            }\n",
    "#             self.M = {\n",
    "#                 'W': np.zeros((n, self.num_classes)),\n",
    "#                 'b': np.zeros((1, self.num_classes))\n",
    "#             }\n",
    "#             self.V = {\n",
    "#                 'W': np.zeros((n, self.num_classes)),\n",
    "#                 'b': np.zeros((1, self.num_classes))\n",
    "#             }\n",
    "\n",
    "        cnt = 1\n",
    "        \n",
    "        # main loop: how much iterate on entire dataset.\n",
    "        for epoch in range(self.max_iter):\n",
    "            # before dive into SGD, shuffle dataset\n",
    "            X_shuffled, y_shuffled = self.shuffle(X, y_encoded)\n",
    "            \n",
    "            # cost variable for printing/logging\n",
    "            avg_loss = 0\n",
    "            \n",
    "            # batch_count = dataset_size / batch_size\n",
    "            batch_count = int(np.ceil(np.shape(X)[0] / self.batch_size))\n",
    "            \n",
    "            # mini-batch loop\n",
    "            for t in range(batch_count):\n",
    "                # draw the {batch_size} number of samples from X and y\n",
    "                X_batch, y_batch, bs = self.next_batch(X_shuffled, y_shuffled, t)\n",
    "                \n",
    "                # just in case, reshape batch of X and y into proper shape.\n",
    "                X_batch = np.reshape(X_batch, (bs, n))\n",
    "                y_batch = np.reshape(y_batch, (bs, self.num_classes))\n",
    "                \n",
    "                # prediction phase\n",
    "                Z = self.forward_prop(X_batch)\n",
    "                Z = np.reshape(Z, (bs, self.num_classes))\n",
    "                \n",
    "                # compute cost phase\n",
    "                loss = self.compute_cost(y_batch, Z)\n",
    "                \n",
    "                # update weights phase\n",
    "                self.backward_prop(X_batch, y_batch, Z, bs, cnt)\n",
    "                \n",
    "                # accumulate loss\n",
    "                avg_loss += loss\n",
    "                cnt += 1\n",
    "        \n",
    "            # logging\n",
    "            avg_loss /= batch_count\n",
    "            if epoch % (self.max_iter / 10) == 0:\n",
    "                print('Cost at epoch {0}: {1}'.format(epoch, avg_loss))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def encode_y(self, y):\n",
    "        y_encoded = np.ones((np.shape(y)[0], self.num_classes)) #1로 이루어진 배열(60000,10)\n",
    "        \n",
    "        for i in range(self.num_classes):\n",
    "            y_encoded[:, i][y != i] = -1\n",
    "            \n",
    "        return y_encoded\n",
    "    \n",
    "    def shuffle(self, X, y):\n",
    "        \"\"\"\n",
    "        Random selection is required for SGD.\n",
    "        But, my approach is to shuffle entire data before every iteration.\n",
    "        This has same effect as random selection.\n",
    "        \n",
    "        Arguments:\n",
    "        ---------------------------\n",
    "        X: images (BATCH_SIZE, 784)\n",
    "        y: labels (BATCH_SIZE, 1)\n",
    "        \n",
    "        Returns:\n",
    "        ---------------------------\n",
    "        shuffled data\n",
    "        \"\"\"\n",
    "        \n",
    "        # the number of dataset samples\n",
    "        m = np.shape(X)[0]\n",
    "        \n",
    "        # variable for shuffle\n",
    "        r = np.arange(0, m)\n",
    "        \n",
    "        np.random.shuffle(r)\n",
    "        \n",
    "        return X[r], y[r]\n",
    "    \n",
    "    def next_batch(self, X, y, t):\n",
    "        \"\"\"\n",
    "        Get next batch.\n",
    "        If it is SGD, next_batch function just pick one sample from dataset.\n",
    "        \n",
    "        Arguments:\n",
    "        ---------------------------------\n",
    "        X: images (60000, 784)\n",
    "        y: labels (60000, 1)\n",
    "        \n",
    "        Returns:\n",
    "        ---------------------------------\n",
    "        X_batch: small subset of X (BATCH_SIZE, 784)\n",
    "        y_batch: small subset of y (BATCH_SIZE, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # the number of dataset samples\n",
    "        m = np.shape(X)[0] #60000\n",
    "        \n",
    "        # draw the {batch_size} number of samples from X and y\n",
    "        X_batch = X[t * self.batch_size : min(m, (t + 1) * self.batch_size)]\n",
    "        y_batch = y[t * self.batch_size : min(m, (t + 1) * self.batch_size)]\n",
    "        bs = min(m, (t + 1) * self.batch_size) - t * self.batch_size\n",
    "        \n",
    "        return X_batch, y_batch, bs\n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        \"\"\"\n",
    "        Process of inference (prediction).\n",
    "        \n",
    "        Arguments:\n",
    "        -----------------------\n",
    "        X: images e.g (BATCH_SIZE, 784)\n",
    "        params: weights dictionary(map in other programming language)\n",
    "        \n",
    "        Returns:\n",
    "        -----------------------\n",
    "        A: \n",
    "        \"\"\"\n",
    "        \n",
    "        # prediction\n",
    "        Z = np.matmul(X, self.params['W']) + self.params['b']\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "#     def sigmoid(self, Z):\n",
    "#         \"\"\"\n",
    "#         sigmoid activation for binary classification\n",
    "        \n",
    "#         Arguments:\n",
    "#         ----------------------\n",
    "#         Z: class score (W.T * X)\n",
    "        \n",
    "#         Returns:\n",
    "#         ----------------------\n",
    "#         sigmoid activation\n",
    "#         \"\"\"\n",
    "        \n",
    "#         return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def compute_cost(self, y, Z):\n",
    "        \"\"\"\n",
    "        compute cost function (loss function)\n",
    "        \n",
    "        Arguments:\n",
    "        ------------------------------\n",
    "        y: true label\n",
    "        Z: class score (W.T * X)\n",
    "        \n",
    "        Returns:\n",
    "        ------------------------------\n",
    "        loss: total cost (loss)\n",
    "        \"\"\"\n",
    "        \n",
    "        # compute loss function\n",
    "        temp = 1 - np.multiply(y, Z)\n",
    "        temp[temp < 0] = 0\n",
    "        loss = np.mean(temp)\n",
    "        return loss\n",
    "    \n",
    "    def backward_prop(self, X, y, Z, bs, cnt):\n",
    "        \"\"\"\n",
    "        update weights\n",
    "        \n",
    "        Arguments:\n",
    "        ----------------------------\n",
    "        X: images e.g (BATCH_SIZE, 784)\n",
    "        y: labels e.g (BATCH_SIZE, 1)\n",
    "        Z: class score after forward propagation\n",
    "        params: weights dictionary(map in other programming language)\n",
    "        eta: learning rate\n",
    "        \n",
    "        Returns:\n",
    "        ----------------------------\n",
    "        params: weights dictionary\n",
    "        \"\"\"\n",
    "        \n",
    "        # number of features\n",
    "        n = np.shape(X)[1]\n",
    "        \n",
    "        # differential vector of loss function to update weights\n",
    "        dw = np.zeros(self.params['W'].shape)\n",
    "        db = np.zeros(self.params['b'].shape)\n",
    "        \n",
    "        Z = np.reshape(Z, (bs, self.num_classes))\n",
    "        temp = np.multiply(y, Z)\n",
    "        temp = 1 - temp\n",
    "        \n",
    "        temp[temp <= 0] = 0\n",
    "        temp[temp > 0] = 1\n",
    "        \n",
    "        y_temp = np.multiply(y, temp.reshape(bs, self.num_classes))\n",
    "        \n",
    "        dw = -(1 / bs) * np.matmul(X.T, y_temp) + (1 / self.C) * self.params['W']\n",
    "        db = -(1 / bs) * np.sum(y_temp, axis=0)\n",
    "\n",
    "#         if cnt == 1:\n",
    "#             self.M['W'] = dw\n",
    "#             self.M['b'] = db\n",
    "#         else:\n",
    "#             self.M['W'] = (self.beta1 * self.M['W'] + (1 - self.beta1) * dw)\n",
    "#             self.M['b'] = (self.beta1 * self.M['b'] + (1 - self.beta1) * db)\n",
    "        \n",
    "#         if cnt == 1:\n",
    "#             self.V['W'] = dw ** 2\n",
    "#             self.V['b'] = db ** 2\n",
    "#         else:\n",
    "#             self.V['W'] = (self.beta2 * self.V['W'] + (1 - self.beta2) * (dw ** 2))\n",
    "#             self.V['b'] = (self.beta2 * self.V['b'] + (1 - self.beta2) * (db ** 2))\n",
    "    \n",
    "#         self.M['W'] = (self.beta1 * self.M['W'] + (1 - self.beta1) * dw) / (1 - self.beta1 ** cnt)\n",
    "#         self.M['b'] = (self.beta1 * self.M['b'] + (1 - self.beta1) * db) / (1 - self.beta1 ** cnt)\n",
    "        \n",
    "#         self.V['W'] = (self.beta2 * self.V['W'] + (1 - self.beta2) * np.square(dw)) / (1 - self.beta2 ** cnt)\n",
    "#         self.V['b'] = (self.beta2 * self.V['b'] + (1 - self.beta2) * np.square(db)) / (1 - self.beta2 ** cnt)\n",
    "\n",
    "        # update weights\n",
    "#         self.params['W'] = self.params['W'] - np.divide(self.eta * self.M['W'], np.sqrt(self.V['W']) + self.epsilon)\n",
    "#         self.params['b'] = self.params['b'] - np.divide(self.eta * self.M['b'], np.sqrt(self.V['b']) + self.epsilon)\n",
    "        \n",
    "        self.params['W'] = self.params['W'] - (self.eta / (1 + self.epsilon * cnt)) * dw\n",
    "        self.params['b'] = self.params['b'] - (self.eta / (1 + self.epsilon * cnt)) * db\n",
    "\n",
    "#         self.params['W'] = self.params['W'] - (self.eta / (1 + self.epsilon * cnt)) * dw\n",
    "#         self.params['b'] = self.params['b'] - (self.eta / (1 + self.epsilon * cnt)) * db\n",
    "        \n",
    "        return self.params\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        m = np.shape(X)[0]\n",
    "        \n",
    "        class_score = self.forward_prop(X)\n",
    "        pred = np.argmax(class_score, axis=1)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        pred = self.predict(X)\n",
    "        score = np.mean(pred == y)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0: 0.20668101207912445\n",
      "Cost at epoch 5: 0.19930864461253286\n",
      "Cost at epoch 10: 0.19925636133943408\n",
      "Cost at epoch 15: 0.1992415575053469\n",
      "Cost at epoch 20: 0.19929039183303537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MySVM at 0x7f80828bcdd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mine=MySVM()\n",
    "mine.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MySVM' object has no attribute 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d5e2ba7549dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MySVM' object has no attribute 'test'"
     ]
    }
   ],
   "source": [
    "result=mine.test(X_test,y_test,w,b)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = pd.DataFrame(lr_grid.cv_results_)\n",
    "#result.to_csv(\"cv_result_유현상.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
